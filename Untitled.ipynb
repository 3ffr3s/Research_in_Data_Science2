{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np \n",
    "from Utils import PklsFolder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self,d_k):\n",
    "        super().__init__()\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self,q,k,v, mask = None, dropout = None):\n",
    "\n",
    "        # q,k,v shape : (batch_size,seq_len, d_k or d_v)\n",
    "        score = torch.matmul(q, k.transpose(-1,-2)) / torch.sqrt(self.d_k)\n",
    "       \n",
    "        if mask is not None:\n",
    "            mask = torch.as_tensor(mask, dtype = torch.bool)\n",
    "            score.masked_fill_(mask, -1e9)\n",
    "\n",
    "        score = F.softmax(score, dim = -1)\n",
    "        if dropout is not None:\n",
    "            score = F.dropout(score, p = dropout)\n",
    "            \n",
    "        output = torch.matmul(score,v)\n",
    "        \n",
    "        return score, ouput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model,num_heads, d_k, d_v, dropout = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.WQ = nn.Linear(self.d_model, self.d_k * self.num_heads, bias= False)\n",
    "        self.WK = nn.Linear(self.d_model, self.d_k * self.num_heads, bias= False) \n",
    "        self.WV = nn.Linear(self.d_model, self.d_v * self.num_heads, bias= False)\n",
    "        self.linear = nn.Linear(self.d_v * self.num_heads ,self.d_model, bias = False)\n",
    "        \n",
    "        self.ScaledDotProductAttention = ScaledDotProductAttention(self.d_k)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = dropout) \n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape = d_model, eps=1e-6)  \n",
    "        \n",
    "    def forward(self,input,mask = None):\n",
    "        # input_shape : (batch_size, seq_len, d_model)\n",
    "        batch_size = input.shape[0]\n",
    "        seq_len = input.shape[1]\n",
    "\n",
    "        residual = input\n",
    "\n",
    "        WQ = self.WQ(input).view(batch_size,seq_len,self.num_heads,self.d_k).transpose(1,2) \n",
    "        WK = self.WK(input).view(batch_size,seq_len,self.num_heads,self.d_k).transpose(1,2)\n",
    "        WV = self.WV(input).view(batch_size,seq_len,self.num_heads,self.d_v).transpose(1,2)\n",
    "        # WQ, WK, WV shape : (batch_size, num_heads, seq_len, d_k or d_v)\n",
    "\n",
    "        if mask is not None:   # mask dimension : (batch_size, seq_len, seq_len)\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        score, context = self.ScaledDotProductAttention(WQ,WK,WV, mask)  #context shape : (batch_size, num_heads, seq_len, d_v)\n",
    "        context = context.transpose(1,2).contiguous().view(batch_size,seq_len, self.num_heads * self.d_v)\n",
    "        output = self.linear(context)  # output shape : (batch_size, seq_len, d_model)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        output = self.layer_norm(output + residual)\n",
    "        \n",
    "        return score, output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforward(nn.Module):\n",
    "    def __init__(self, d_model, d_hid, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_hid = d_hid\n",
    "        \n",
    "        self.linear1 = nn.Linear(self.d_model, self.d_hid)\n",
    "        self.linear2 = nn.Linear(self.d_hid, self.d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape = self.d_model, eps=1e-6)\n",
    "    \n",
    "    def forward(self,input):\n",
    "        residual = input\n",
    "        \n",
    "        output = F.relu(self.linear1(input))\n",
    "        output = self.linear2(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.layer_norm(output+residual)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,seq_len,d_model, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        pe = torch.zeros((self.seq_len, self.d_model), requires_grad = False)\n",
    "        self.dropout = nn.Dropout(p =dropout)\n",
    "\n",
    "        for pos in range(self.seq_len):\n",
    "            for i in range(self.d_model):\n",
    "                if i%2 == 0:\n",
    "                    pe[pos][i] = math.sin(pos/(math.pow(10000,i/self.d_model)))\n",
    "                elif i%2 ==1:\n",
    "                    pe[pos][i] = math.cos(pos/(math.pow(10000,(i-1)/self.d_model)))\n",
    "                    \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self,input):\n",
    "        return self.dropout(input + self.pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self,hid_dim,attn_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn1 = nn.Linear(hid_dim, attn_dim)\n",
    "        self.attn2 = nn.Linear(attn_dim,1, bias = False)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self,input):\n",
    "        attn1 = torch.tanh(self.attn1(input))   # attn1 shape : (batch_size, seq_len, attn_dim)\n",
    "        attn2 = self.attn2(attn1).squeeze(-1)               # attn2 shape : (batch_size, seq_len, 1)\n",
    "        attn_score = self.softmax(attn2).unsqueeze(-1)\n",
    "        output = (attn_score * input).sum(dim = 1)\n",
    "        return attn_score, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByteEncoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,num_heads, d_k, d_v, d_hid, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.MultiHeadAttention = MultiHeadAttention(d_model, num_heads, d_k, d_v,dropout = dropout)\n",
    "        self.PositionwiseFeedforward = PositionwiseFeedforward(d_model, d_hid, dropout = dropout)\n",
    "        \n",
    "    def forward(self,input, mask = None):\n",
    "        score, output = self.MultiHeadAttention(input,mask = mask)\n",
    "        output = self.PositionwiseFeedforward(output)\n",
    "        return score, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByteEncoder(nn.Module):\n",
    "    def __init__(self,num_layers, d_model,num_heads, d_k, d_v, d_hid, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.ModuleList([ByteEncoderLayer(d_model,num_heads, d_k, d_v, d_hid, dropout = dropout) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self,input):\n",
    "        output = input\n",
    "        for layer in self.encoder:\n",
    "            output, score = layer(output)\n",
    "        return score, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flow_CLF(nn.Module):\n",
    "    def __init__(self, num_layers, d_model,num_heads, d_k, d_v, d_hid, add_attn_dim, pck_len, num_classes = 2, dropout = 0.1, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.byte_emb = nn.Embedding(256, d_model)\n",
    "        self.positionalEncoder =  PositionalEncoding(pck_len,d_model, dropout = dropout)     #seq_len 이거 다시 설정해줘야 함 \n",
    "        \n",
    "        #Byte\n",
    "        self.ByteEncoder = ByteEncoder(num_layers, d_model,num_heads, d_k, d_v, d_hid, dropout = dropout)\n",
    "        \n",
    "        #Packet\n",
    "        self.PacketEncoder = AdditiveAttention(d_model,add_attn_dim)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def foward(self,input): \n",
    "        batch_size = input.shape[0]\n",
    "        flow_len = input.shape[1]\n",
    "        \n",
    "        byte_embeddings = self.byte_emb(input)\n",
    "        byte_embeddings = self.positionalEncoder(byte_embeddings)\n",
    "        \n",
    "        packet_embeddings = torch.zeros([batch_size,flow_len,d_model]).to(self.device)\n",
    "        for idx, flow in enumerate(byte_embeddings):\n",
    "            score, byte_encoding = self.ByteEncoder(flow)              # output shape : (batch_size,flow_len,pck_len,d_model)\n",
    "            packet_embedding = byte_encoding.mean(dim = 1)\n",
    "            packet_embeddings[idx,:,:] = packet_embedding\n",
    "        \n",
    "        score, output = self.PacketEncoder(packet_embeddings)\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.embedding 부분 그냥 word2vec으로 바꿀까\n",
    "Pkls_Folder에서 flow 길이/packet 길이 패딩 부분 수정해야 됨 + Flow_CLF에서 pck_len 인자 값 바꿔주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "논문에서 3.4 Embedding and Softmax 부분 다시 보기\n",
    "-embedding layer ouput을 루트 d_model로 나눠야 되는건지\n",
    "\n",
    "데이터 전처리 : [] 빈 데이터 삭제하고 flow 구성하는 패킷 개수, 각 패킷의 크기 등 정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout 사용하기 때문에 \n",
    "model.train()\n",
    "model.eval()  이 코드 꼭 넣기"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
